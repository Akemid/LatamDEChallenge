{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEER CHALLENGE\n",
    "\n",
    "Para el desarrollo de este challenge estoy utilizando Python 3.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomprimir el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import unzip_json\n",
    "file_path = \"datasets/tweets.json.zip\"\n",
    "output_dir = \"datasets/\"\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    unzip_json(file_path,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solución de las preguntas de optimización de memoria se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar la memoria más no mantenerla al minimo.\n",
    "2. El tiempo de ejecución no debería ser excesivo aunque se este buscando optimizar la memoria.\n",
    "3. El archivo es un ejemplo pero pueden haber casos dónde sean muy extensos.\n",
    "\n",
    "Como solución se ha considerado utilizar la librería Pandas debido a su fácil manejo y que nos provee agregar argumentos a la función de lectura del archivo JSON para poder optimizar la memoría. Por otro lado, se manejó la alternativa de utilizar una lectura linea a linea del archivo; sin embargo, esto tomaría demasiado tiempo de ejecución en relación al beneficio de ahorrar memoría.\n",
    "\n",
    "Consideraciones claves para reducir el consumo de memoría con Pandas:\n",
    "\n",
    "1. Chunksize: Nos permite dividir archivos grandes en pequeños trozos lo cual permite el ahorro de memoría.\n",
    "2. dtype: Nos permite definir los tipos de datos de cada columna, así se puede asignar tipos de datos menores a los de por defecto. En este caso, tambien se uso para convertir información no necesaria en un tipo de dato booleano ( dado que cuesta mucho menos).\n",
    "3. Elegir las columnas necesarias para el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solución de las preguntas de optimizacion de tiempo se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar el tiempo de ejecución sin dejar de lado el consumo de memoria\n",
    "2. Limites de hardware propio.\n",
    "3. La data suministrada no será muy extensa para considerarse BigData.\n",
    "\n",
    "Como solución para las preguntas de optimización de tiempo se ha considerado entre dos alternativas la utilización de la libreria Polars y la lectura de cada linea utilizando la libreria JSON. Sabemos que para optimizar el tiempo de ejecución es necesario tambien tener en cuenta la memoria consumida, además de la escritura de código eficiente. Por otro lado, la paralelización de las tareas ayuda a acelerar el proceso aunque en Python es complicado debido al GIL existente. Asimismo, el lenguaje interpretado como tal es más lento que lenguajes compiladas como Rust o C++, por tal motivo, para acelerar la ejecución se han buscado opciones de librerias que tengan una API para python. Por ultimo, se ha tenido en cuenta que las operaciones vectoriales también aumentan la velocidad de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por todo lo anterior descrito, Polars es la mejor alternativa para optimizar la lectura de los dataframes (según las consideraciones descritas más arriba).\n",
    "\n",
    "Consideraciones de [Polars](https://docs.pola.rs/):\n",
    "\n",
    "1. Escrito en Rust.\n",
    "2. Trabajo en paralelo.\n",
    "3. Motor de Consultas Vectorizadas.\n",
    "\n",
    "Consideraciones del código:\n",
    "\n",
    "1. Se ha escrito el codigo utilizando lazy API puesto que permite trabajar con datasets grandes y en streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8 137.3125 MiB 137.3125 MiB           1   @profile(precision=4)\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             # The selected function is the one that has the best performance,\n",
      "    11                                             # It is the one that uses pandas with the best memory optimization.\n",
      "    12                                             # The line by line function is the one that uses the least memory but it is the slowest\n",
      "    13                                             # And for that reason it is not the best option.\n",
      "    14                                             \n",
      "    15                                             # OPTIMIZATION:\n",
      "    16                                             # Many options to optimize the memory\n",
      "    17                                             # Chunksize allows to read the file in chunks\n",
      "    18                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    19                                             # in order to save memory\n",
      "    20 137.4375 MiB   0.1250 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=1000,convert_dates=True,dtype={\n",
      "    21 137.3125 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    22 137.3125 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    23 137.3125 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    24 137.3125 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    25 137.3125 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    26 137.3125 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    27 137.3125 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    28 137.3125 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    29 137.3125 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    30 137.3125 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    31 137.3125 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    32 137.3125 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    33 137.3125 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    34 137.3125 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    35 137.3125 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    36 137.3125 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    37 137.3125 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    38 137.3125 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    39 137.3125 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    40 137.3125 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    41 137.3125 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    42                                             })\n",
      "    43                                             # Each chunk is processed and then added to the result\n",
      "    44 137.4375 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    45                                             # The result is reduced to a single dataframe\n",
      "    46 171.9766 MiB  34.5391 MiB           1       result = reduce(add, procced_tweets)\n",
      "    47                                             # The top dates are extracted\n",
      "    48 172.2266 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    49 172.2266 MiB   0.0000 MiB           1       users = []\n",
      "    50 172.2266 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    51                                             # The top user for each date is extracted \n",
      "    52 172.2266 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    53                                                 # Filter by date\n",
      "    54 172.2266 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    55                                                 # Get the sum of the tweets for each user in the date\n",
      "    56 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    57                                                 # Get the user with the most tweets\n",
      "    58 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "    59                                                 # Get the username\n",
      "    60 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "    61 172.2266 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "    62 172.2266 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory\n",
    "result = q1_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 TIME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 s, sys: 789 ms, total: 5.95 s\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q1_time import q1_time\n",
    "result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.69 s ± 285 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q1_time import q1_time\n",
    "%timeit result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los top 10 emojis más usados con su respectivo conteo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    10 145.6875 MiB 145.6875 MiB           1   @profile(precision=4)\n",
      "    11                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    12 151.0625 MiB   0.0000 MiB           2       with open(file_path, \"r\") as file:\n",
      "    13 145.6875 MiB   0.0000 MiB           1           emoji_list_total = []\n",
      "    14 151.0625 MiB   0.2500 MiB      117408           for line in file:\n",
      "    15 151.0625 MiB   3.3750 MiB      117407               tweet = json.loads(line)\n",
      "    16 151.0625 MiB   1.5000 MiB      117407               emoji_list_extracted = emoji.emoji_list(tweet[\"content\"])\n",
      "    17 151.0625 MiB   0.0000 MiB      395143               emoji_list_extracted = [emoji[\"emoji\"] for emoji in emoji_list_extracted]\n",
      "    18 151.0625 MiB   0.2500 MiB      117407               emoji_list_total.extend(emoji_list_extracted)\n",
      "    19 151.0625 MiB   0.0000 MiB           1           counter_emojis = Counter(emoji_list_total)\n",
      "    20 151.0625 MiB   0.0000 MiB           1           top_emojis = counter_emojis.most_common(10)\n",
      "    21 151.0625 MiB   0.0000 MiB           1           return top_emojis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory\n",
    "result = q2_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    22 155.2891 MiB 155.2891 MiB           1   @profile(precision=4)\n",
      "    23                                         def q2_memory_pandas_map_reduce(file_path: str) -> List[Tuple[str, int]]:\n",
      "    24 155.2891 MiB   0.0000 MiB           2       tweets = pd.read_json(\n",
      "    25 155.2891 MiB   0.0000 MiB           1           file_path,\n",
      "    26 155.2891 MiB   0.0000 MiB           1           lines=True,\n",
      "    27 155.2891 MiB   0.0000 MiB           1           chunksize=1000,\n",
      "    28 155.2891 MiB   0.0000 MiB           1           convert_dates=True,\n",
      "    29 155.2891 MiB   0.0000 MiB          19           dtype={\n",
      "    30 155.2891 MiB   0.0000 MiB           1               \"url\": \"bool\",\n",
      "    31 155.2891 MiB   0.0000 MiB           1               \"date\": \"bool\",\n",
      "    32 155.2891 MiB   0.0000 MiB           1               \"content\": \"string\",\n",
      "    33 155.2891 MiB   0.0000 MiB           1               \"renderedContent\": \"bool\",\n",
      "    34 155.2891 MiB   0.0000 MiB           1               \"id\": \"bool\",\n",
      "    35 155.2891 MiB   0.0000 MiB           1               \"user\": \"bool\",\n",
      "    36 155.2891 MiB   0.0000 MiB           1               \"outlinks\": \"bool\",\n",
      "    37 155.2891 MiB   0.0000 MiB           1               \"tcooutlinks\": \"bool\",\n",
      "    38 155.2891 MiB   0.0000 MiB           1               \"replyCount\": \"bool\",\n",
      "    39 155.2891 MiB   0.0000 MiB           1               \"retweetCount\": \"bool\",\n",
      "    40 155.2891 MiB   0.0000 MiB           1               \"likeCount\": \"bool\",\n",
      "    41 155.2891 MiB   0.0000 MiB           1               \"quoteCount\": \"bool\",\n",
      "    42 155.2891 MiB   0.0000 MiB           1               \"conversationId\": \"bool\",\n",
      "    43 155.2891 MiB   0.0000 MiB           1               \"lang\": \"bool\",\n",
      "    44 155.2891 MiB   0.0000 MiB           1               \"source\": \"bool\",\n",
      "    45 155.2891 MiB   0.0000 MiB           1               \"sourceUrl\": \"bool\",\n",
      "    46 155.2891 MiB   0.0000 MiB           1               \"sourceLabel\": \"bool\",\n",
      "    47 155.2891 MiB   0.0000 MiB           1               \"media\": \"bool\",\n",
      "    48 155.2891 MiB   0.0000 MiB           1               \"retweetedTweet\": \"bool\",\n",
      "    49 155.2891 MiB   0.0000 MiB           1               \"quotedTweet\": \"bool\",\n",
      "    50 155.2891 MiB   0.0000 MiB           1               \"mentionedUsers\": \"bool\",\n",
      "    51                                                 },\n",
      "    52                                             )\n",
      "    53                                         \n",
      "    54 155.2891 MiB   0.0000 MiB           1       emoji_list = map(get_emojis, tweets)\n",
      "    55 184.9141 MiB  29.6250 MiB         235       result = reduce(lambda x,y: x+y, emoji_list)\n",
      "    56 184.9141 MiB   0.0000 MiB           1       counter_emojis = Counter(result)\n",
      "    57 184.9141 MiB   0.0000 MiB           1       top_emojis = counter_emojis.most_common(10)\n",
      "    58 184.9141 MiB   0.0000 MiB           1       return top_emojis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory_pandas_map_reduce\n",
    "result = q2_memory_pandas_map_reduce(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.2 s, sys: 986 ms, total: 47.2 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q2_memory import q2_memory_pandas_map_reduce\n",
    "\n",
    "result = q2_memory_pandas_map_reduce(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 228 ms, total: 29.3 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q2_memory import q2_memory\n",
    "result = q2_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
