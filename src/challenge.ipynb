{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEER CHALLENGE\n",
    "\n",
    "Para el desarrollo de este challenge estoy utilizando Python 3.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomprimir el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import unzip_json\n",
    "file_path = \"datasets/tweets.json.zip\"\n",
    "output_dir = \"datasets/\"\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    unzip_json(file_path,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solución de esta pregunta se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar la memoria más no mantenerla al minimo.\n",
    "2. El tiempo de ejecución no debería ser excesivo aunque se este buscando optimizar la memoria.\n",
    "\n",
    "Como solución se ha considerado utilizar la librería Pandas debido a su fácil manejo y que nos provee agregar argumentos a la función de lectura del archivo JSON para poder optimizar la memoría. Por otro lado, se manejó la alternativa de utilizar una lectura linea a linea del archivo; sin embargo, esto tomaría demasiado tiempo de ejecución en relación al beneficio de ahorrar memoría.\n",
    "\n",
    "Consideraciones claves para reducir el consumo de memoría con Pandas:\n",
    "\n",
    "1. Chunksize: Nos permite dividir archivos grandes en pequeños trozos lo cual permite el ahorro de memoría.\n",
    "2. dtype: Nos permite definir los tipos de datos de cada columna, así se puede asignar tipos de datos menores a los de por defecto. En este caso, tambien se uso para convertir información no necesaria en un tipo de dato booleano ( dado que cuesta mucho menos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8 137.3125 MiB 137.3125 MiB           1   @profile(precision=4)\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             # The selected function is the one that has the best performance,\n",
      "    11                                             # It is the one that uses pandas with the best memory optimization.\n",
      "    12                                             # The line by line function is the one that uses the least memory but it is the slowest\n",
      "    13                                             # And for that reason it is not the best option.\n",
      "    14                                             \n",
      "    15                                             # OPTIMIZATION:\n",
      "    16                                             # Many options to optimize the memory\n",
      "    17                                             # Chunksize allows to read the file in chunks\n",
      "    18                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    19                                             # in order to save memory\n",
      "    20 137.4375 MiB   0.1250 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=1000,convert_dates=True,dtype={\n",
      "    21 137.3125 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    22 137.3125 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    23 137.3125 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    24 137.3125 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    25 137.3125 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    26 137.3125 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    27 137.3125 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    28 137.3125 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    29 137.3125 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    30 137.3125 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    31 137.3125 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    32 137.3125 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    33 137.3125 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    34 137.3125 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    35 137.3125 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    36 137.3125 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    37 137.3125 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    38 137.3125 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    39 137.3125 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    40 137.3125 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    41 137.3125 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    42                                             })\n",
      "    43                                             # Each chunk is processed and then added to the result\n",
      "    44 137.4375 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    45                                             # The result is reduced to a single dataframe\n",
      "    46 171.9766 MiB  34.5391 MiB           1       result = reduce(add, procced_tweets)\n",
      "    47                                             # The top dates are extracted\n",
      "    48 172.2266 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    49 172.2266 MiB   0.0000 MiB           1       users = []\n",
      "    50 172.2266 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    51                                             # The top user for each date is extracted \n",
      "    52 172.2266 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    53                                                 # Filter by date\n",
      "    54 172.2266 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    55                                                 # Get the sum of the tweets for each user in the date\n",
      "    56 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    57                                                 # Get the user with the most tweets\n",
      "    58 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "    59                                                 # Get the username\n",
      "    60 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "    61 172.2266 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "    62 172.2266 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = q1_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
