{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEER CHALLENGE\n",
    "\n",
    "Para el desarrollo de este challenge estoy utilizando Python 3.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomprimir el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import unzip_json\n",
    "file_path = \"datasets/tweets.json.zip\"\n",
    "output_dir = \"datasets/\"\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    unzip_json(file_path,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "Primero se intentó agregar todo el json en un dataframe en pandas, reduciendo el uso de memoria utilizando el chunksize y solo dejando las columnas que eran necesarias. Se probaron con distintos chunksize para ver el cambio del uso de la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import q1_memory_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    55 151.9844 MiB 151.9844 MiB           1   @profile(precision=4)\n",
      "    56                                         def q1_memory_pandas(file_path: str,chunksize=1000) -> List[Tuple[datetime.date, str]]:\n",
      "    57                                             # Many options to optimize the memory\n",
      "    58                                             # Chunksize allows to read the file in chunks\n",
      "    59                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    60                                             # in order to save memory\n",
      "    61 152.1094 MiB   0.1250 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=chunksize,convert_dates=True,dtype={\n",
      "    62 151.9844 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    63 151.9844 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    64 151.9844 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    65 151.9844 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    66 151.9844 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    67 151.9844 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    68 151.9844 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    69 151.9844 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    70 151.9844 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    71 151.9844 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    72 151.9844 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    73 151.9844 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    74 151.9844 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    75 151.9844 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    76 151.9844 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    77 151.9844 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    78 151.9844 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    79 151.9844 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    80 151.9844 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    81 151.9844 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    82 151.9844 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    83                                             })\n",
      "    84                                             # Each chunk is processed and then added to the result\n",
      "    85 152.1094 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    86                                             # The result is reduced to a single dataframe\n",
      "    87 186.4844 MiB  34.3750 MiB           1       result = reduce(add, procced_tweets)\n",
      "    88                                             # The top dates are extracted\n",
      "    89 186.8594 MiB   0.3750 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    90 186.8594 MiB   0.0000 MiB           1       users = []\n",
      "    91 186.8594 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    92                                             # The top user for each date is extracted \n",
      "    93 186.8594 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    94                                                 # Filter by date\n",
      "    95 186.8594 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    96                                                 # Get the sum of the tweets for each user in the date\n",
      "    97 186.8594 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    98                                                 # Get the user with the most tweets\n",
      "    99 186.8594 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "   100                                                 # Get the username\n",
      "   101 186.8594 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "   102 186.8594 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "   103 186.8594 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = q1_memory_pandas(output_file_path,chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    55 157.2539 MiB 157.2539 MiB           1   @profile(precision=4)\n",
      "    56                                         def q1_memory_pandas(file_path: str,chunksize=1000) -> List[Tuple[datetime.date, str]]:\n",
      "    57                                             # Many options to optimize the memory\n",
      "    58                                             # Chunksize allows to read the file in chunks\n",
      "    59                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    60                                             # in order to save memory\n",
      "    61 157.2539 MiB   0.0000 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=chunksize,convert_dates=True,dtype={\n",
      "    62 157.2539 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    63 157.2539 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    64 157.2539 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    65 157.2539 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    66 157.2539 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    67 157.2539 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    68 157.2539 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    69 157.2539 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    70 157.2539 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    71 157.2539 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    72 157.2539 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    73 157.2539 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    74 157.2539 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    75 157.2539 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    76 157.2539 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    77 157.2539 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    78 157.2539 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    79 157.2539 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    80 157.2539 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    81 157.2539 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    82 157.2539 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    83                                             })\n",
      "    84                                             # Each chunk is processed and then added to the result\n",
      "    85 157.2539 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    86                                             # The result is reduced to a single dataframe\n",
      "    87 214.9219 MiB  57.6680 MiB           1       result = reduce(add, procced_tweets)\n",
      "    88                                             # The top dates are extracted\n",
      "    89 215.1719 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    90 215.1719 MiB   0.0000 MiB           1       users = []\n",
      "    91 215.1719 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    92                                             # The top user for each date is extracted \n",
      "    93 215.1719 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    94                                                 # Filter by date\n",
      "    95 215.1719 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    96                                                 # Get the sum of the tweets for each user in the date\n",
      "    97 215.1719 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    98                                                 # Get the user with the most tweets\n",
      "    99 215.1719 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "   100                                                 # Get the username\n",
      "   101 215.1719 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "   102 215.1719 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "   103 215.1719 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = q1_memory_pandas(output_file_path,chunksize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    55 152.0195 MiB 152.0195 MiB           1   @profile(precision=4)\n",
      "    56                                         def q1_memory_pandas(file_path: str,chunksize=1000) -> List[Tuple[datetime.date, str]]:\n",
      "    57                                             # Many options to optimize the memory\n",
      "    58                                             # Chunksize allows to read the file in chunks\n",
      "    59                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    60                                             # in order to save memory\n",
      "    61 152.0195 MiB   0.0000 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=chunksize,convert_dates=True,dtype={\n",
      "    62 152.0195 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    63 152.0195 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    64 152.0195 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    65 152.0195 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    66 152.0195 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    67 152.0195 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    68 152.0195 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    69 152.0195 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    70 152.0195 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    71 152.0195 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    72 152.0195 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    73 152.0195 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    74 152.0195 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    75 152.0195 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    76 152.0195 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    77 152.0195 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    78 152.0195 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    79 152.0195 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    80 152.0195 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    81 152.0195 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    82 152.0195 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    83                                             })\n",
      "    84                                             # Each chunk is processed and then added to the result\n",
      "    85 152.0195 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    86                                             # The result is reduced to a single dataframe\n",
      "    87 175.5039 MiB  23.4844 MiB           1       result = reduce(add, procced_tweets)\n",
      "    88                                             # The top dates are extracted\n",
      "    89 175.7539 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    90 175.7539 MiB   0.0000 MiB           1       users = []\n",
      "    91 175.7539 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    92                                             # The top user for each date is extracted \n",
      "    93 175.7539 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    94                                                 # Filter by date\n",
      "    95 175.7539 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    96                                                 # Get the sum of the tweets for each user in the date\n",
      "    97 175.7539 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    98                                                 # Get the user with the most tweets\n",
      "    99 175.7539 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "   100                                                 # Get the username\n",
      "   101 175.7539 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "   102 175.7539 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "   103 175.7539 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = q1_memory_pandas(output_file_path,chunksize=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    55 152.1758 MiB 152.1758 MiB           1   @profile(precision=4)\n",
      "    56                                         def q1_memory_pandas(file_path: str,chunksize=1000) -> List[Tuple[datetime.date, str]]:\n",
      "    57                                             # Many options to optimize the memory\n",
      "    58                                             # Chunksize allows to read the file in chunks\n",
      "    59                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    60                                             # in order to save memory\n",
      "    61 152.4258 MiB   0.2500 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=chunksize,convert_dates=True,dtype={\n",
      "    62 152.1758 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    63 152.1758 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    64 152.1758 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    65 152.1758 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    66 152.1758 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    67 152.1758 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    68 152.1758 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    69 152.1758 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    70 152.1758 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    71 152.1758 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    72 152.1758 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    73 152.1758 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    74 152.1758 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    75 152.1758 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    76 152.1758 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    77 152.1758 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    78 152.1758 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    79 152.1758 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    80 152.1758 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    81 152.1758 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    82 152.1758 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    83                                             })\n",
      "    84                                             # Each chunk is processed and then added to the result\n",
      "    85 152.4258 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    86                                             # The result is reduced to a single dataframe\n",
      "    87 170.0508 MiB  17.6250 MiB           1       result = reduce(add, procced_tweets)\n",
      "    88                                             # The top dates are extracted\n",
      "    89 170.3008 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    90 170.3008 MiB   0.0000 MiB           1       users = []\n",
      "    91 170.3008 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    92                                             # The top user for each date is extracted \n",
      "    93 170.3008 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    94                                                 # Filter by date\n",
      "    95 170.3008 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    96                                                 # Get the sum of the tweets for each user in the date\n",
      "    97 170.3008 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    98                                                 # Get the user with the most tweets\n",
      "    99 170.3008 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "   100                                                 # Get the username\n",
      "   101 170.3008 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "   102 170.3008 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "   103 170.3008 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = q1_memory_pandas(output_file_path,chunksize=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linea por linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    23 152.2227 MiB 152.2227 MiB           1   @profile(precision=4)\n",
      "    24                                         def q1_memory_line_by_line(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    25                                             # We need to optimize the memory\n",
      "    26                                             # Read line by line using json\n",
      "    27 156.3477 MiB   0.0000 MiB          27       tweets_dict = defaultdict(lambda: defaultdict(int))\n",
      "    28 157.0977 MiB   0.1250 MiB           2       with open(file_path, 'r', encoding='utf-8') as file:\n",
      "    29                                                 # Read line by line\n",
      "    30 157.0977 MiB   0.2500 MiB      117408           for line in file:\n",
      "    31                                                     # Load the json\n",
      "    32 157.0977 MiB   3.2500 MiB      117407               tweet = json.loads(line)\n",
      "    33                                                     # Extract the date and convert into date \n",
      "    34 157.0977 MiB   0.0000 MiB      117407               date = datetime.strptime(tweet['date'], \"%Y-%m-%dT%H:%M:%S%z\").date()\n",
      "    35                                                     # Extract the username\n",
      "    36 157.0977 MiB   0.0000 MiB      117407               username = tweet['user']['username']\n",
      "    37                                                     # Count by date and username\n",
      "    38 157.0977 MiB   1.2500 MiB      117407               tweets_dict[date][username] += 1\n",
      "    39                                             # Get all the data in form of item (date, {user:count})\n",
      "    40 157.0977 MiB   0.0000 MiB           1       tweet_dict_items = tweets_dict.items()\n",
      "    41                                             # Sort the dates by the sum of the tweets counts of each user and order desc\n",
      "    42 157.0977 MiB   0.0000 MiB          27       tweets_dict = sorted(tweet_dict_items, key=lambda x:sum(x[1].values()), reverse=True)\n",
      "    43                                             # Get the top 10 dates\n",
      "    44 157.0977 MiB   0.0000 MiB           1       tweets_dict = tweets_dict[:10]\n",
      "    45                                             # Get in dict format\n",
      "    46 157.0977 MiB   0.0000 MiB           1       tweets_dict = dict(tweets_dict)\n",
      "    47                                             # Creating a list with comprehension in order to save memory\n",
      "    48 157.0977 MiB   0.0000 MiB          11       users_top_tweets = [\n",
      "    49                                                                 # Get the user with the most tweets for each date\n",
      "    50 157.0977 MiB   0.0000 MiB       88328                           max(tweets_dict[date],key=lambda x: tweets_dict[date][x])\n",
      "    51 157.0977 MiB   0.0000 MiB          11                           for date in tweets_dict.keys()\n",
      "    52                                                                 ]        \n",
      "    53 157.0977 MiB   0.0000 MiB           1       return list(zip(tweets_dict.keys(), users_top_tweets))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory_line_by_line\n",
    "\n",
    "result = q1_memory_line_by_line(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
