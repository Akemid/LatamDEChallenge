{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluciÃ³n y todas las suposiciones que estÃ¡s considerando. AquÃ­ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEER CHALLENGE\n",
    "\n",
    "Para el desarrollo de este challenge estoy utilizando Python 3.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomprimir el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import unzip_json\n",
    "file_path = \"datasets/tweets.json.zip\"\n",
    "output_dir = \"datasets/\"\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    unzip_json(file_path,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la soluciÃ³n de las preguntas de optimizaciÃ³n de memoria se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar la memoria mÃ¡s no mantenerla al minimo.\n",
    "2. El tiempo de ejecuciÃ³n no deberÃ­a ser excesivo aunque se este buscando optimizar la memoria.\n",
    "3. El archivo es un ejemplo pero pueden haber casos dÃ³nde sean muy extensos.\n",
    "\n",
    "Como soluciÃ³n se ha considerado utilizar la librerÃ­a Pandas debido a su fÃ¡cil manejo y que nos provee agregar argumentos a la funciÃ³n de lectura del archivo JSON para poder optimizar la memorÃ­a. Por otro lado, se manejÃ³ la alternativa de utilizar una lectura linea a linea del archivo; sin embargo, esto tomarÃ­a demasiado tiempo de ejecuciÃ³n en relaciÃ³n al beneficio de ahorrar memorÃ­a.\n",
    "\n",
    "Consideraciones claves para reducir el consumo de memorÃ­a con Pandas:\n",
    "\n",
    "1. Chunksize: Nos permite dividir archivos grandes en pequeÃ±os trozos lo cual permite el ahorro de memorÃ­a.\n",
    "2. dtype: Nos permite definir los tipos de datos de cada columna, asÃ­ se puede asignar tipos de datos menores a los de por defecto. En este caso, tambien se uso para convertir informaciÃ³n no necesaria en un tipo de dato booleano ( dado que cuesta mucho menos).\n",
    "3. Elegir las columnas necesarias para el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la soluciÃ³n de las preguntas de optimizacion de tiempo se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar el tiempo de ejecuciÃ³n sin dejar de lado el consumo de memoria\n",
    "2. Limites de hardware propio.\n",
    "3. La data suministrada no serÃ¡ muy extensa para considerarse BigData.\n",
    "\n",
    "Como soluciÃ³n para las preguntas de optimizaciÃ³n de tiempo se ha considerado entre dos alternativas la utilizaciÃ³n de la libreria Polars y la lectura de cada linea utilizando la libreria JSON. Sabemos que para optimizar el tiempo de ejecuciÃ³n es necesario tambien tener en cuenta la memoria consumida, ademÃ¡s de la escritura de cÃ³digo eficiente. Por otro lado, la paralelizaciÃ³n de las tareas ayuda a acelerar el proceso aunque en Python es complicado debido al GIL existente. Asimismo, el lenguaje interpretado como tal es mÃ¡s lento que lenguajes compiladas como Rust o C++, por tal motivo, para acelerar la ejecuciÃ³n se han buscado opciones de librerias que tengan una API para python. Por ultimo, se ha tenido en cuenta que las operaciones vectoriales tambiÃ©n aumentan la velocidad de ejecuciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por todo lo anterior descrito, Polars es la mejor alternativa para optimizar la lectura de los dataframes (segÃºn las consideraciones descritas mÃ¡s arriba).\n",
    "\n",
    "Consideraciones de [Polars](https://docs.pola.rs/):\n",
    "\n",
    "1. Escrito en Rust.\n",
    "2. Trabajo en paralelo.\n",
    "3. Motor de Consultas Vectorizadas.\n",
    "\n",
    "Consideraciones del cÃ³digo:\n",
    "\n",
    "1. Se ha escrito el codigo utilizando lazy API puesto que permite trabajar con datasets grandes y en streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las top 10 fechas donde hay mÃ¡s tweets. Mencionar el usuario (username) que mÃ¡s publicaciones tiene por cada uno de esos dÃ­as. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8 137.3125 MiB 137.3125 MiB           1   @profile(precision=4)\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             # The selected function is the one that has the best performance,\n",
      "    11                                             # It is the one that uses pandas with the best memory optimization.\n",
      "    12                                             # The line by line function is the one that uses the least memory but it is the slowest\n",
      "    13                                             # And for that reason it is not the best option.\n",
      "    14                                             \n",
      "    15                                             # OPTIMIZATION:\n",
      "    16                                             # Many options to optimize the memory\n",
      "    17                                             # Chunksize allows to read the file in chunks\n",
      "    18                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    19                                             # in order to save memory\n",
      "    20 137.4375 MiB   0.1250 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=1000,convert_dates=True,dtype={\n",
      "    21 137.3125 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    22 137.3125 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    23 137.3125 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    24 137.3125 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    25 137.3125 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    26 137.3125 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    27 137.3125 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    28 137.3125 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    29 137.3125 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    30 137.3125 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    31 137.3125 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    32 137.3125 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    33 137.3125 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    34 137.3125 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    35 137.3125 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    36 137.3125 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    37 137.3125 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    38 137.3125 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    39 137.3125 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    40 137.3125 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    41 137.3125 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    42                                             })\n",
      "    43                                             # Each chunk is processed and then added to the result\n",
      "    44 137.4375 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    45                                             # The result is reduced to a single dataframe\n",
      "    46 171.9766 MiB  34.5391 MiB           1       result = reduce(add, procced_tweets)\n",
      "    47                                             # The top dates are extracted\n",
      "    48 172.2266 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    49 172.2266 MiB   0.0000 MiB           1       users = []\n",
      "    50 172.2266 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    51                                             # The top user for each date is extracted \n",
      "    52 172.2266 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    53                                                 # Filter by date\n",
      "    54 172.2266 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    55                                                 # Get the sum of the tweets for each user in the date\n",
      "    56 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    57                                                 # Get the user with the most tweets\n",
      "    58 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "    59                                                 # Get the username\n",
      "    60 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "    61 172.2266 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "    62 172.2266 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory\n",
    "result = q1_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 TIME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 s, sys: 789 ms, total: 5.95 s\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q1_time import q1_time\n",
    "result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.69 s Â± 285 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q1_time import q1_time\n",
    "%timeit result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los top 10 emojis mÃ¡s usados con su respectivo conteo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    10 145.6875 MiB 145.6875 MiB           1   @profile(precision=4)\n",
      "    11                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    12 151.0625 MiB   0.0000 MiB           2       with open(file_path, \"r\") as file:\n",
      "    13 145.6875 MiB   0.0000 MiB           1           emoji_list_total = []\n",
      "    14 151.0625 MiB   0.2500 MiB      117408           for line in file:\n",
      "    15 151.0625 MiB   3.3750 MiB      117407               tweet = json.loads(line)\n",
      "    16 151.0625 MiB   1.5000 MiB      117407               emoji_list_extracted = emoji.emoji_list(tweet[\"content\"])\n",
      "    17 151.0625 MiB   0.0000 MiB      395143               emoji_list_extracted = [emoji[\"emoji\"] for emoji in emoji_list_extracted]\n",
      "    18 151.0625 MiB   0.2500 MiB      117407               emoji_list_total.extend(emoji_list_extracted)\n",
      "    19 151.0625 MiB   0.0000 MiB           1           counter_emojis = Counter(emoji_list_total)\n",
      "    20 151.0625 MiB   0.0000 MiB           1           top_emojis = counter_emojis.most_common(10)\n",
      "    21 151.0625 MiB   0.0000 MiB           1           return top_emojis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory\n",
    "result = q2_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ™', 5049),\n",
       " ('ğŸ˜‚', 3072),\n",
       " ('ğŸšœ', 2972),\n",
       " ('ğŸŒ¾', 2182),\n",
       " ('ğŸ‡®ğŸ‡³', 2086),\n",
       " ('ğŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ğŸ™ğŸ»', 1317),\n",
       " ('ğŸ’š', 1040)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    22 155.2891 MiB 155.2891 MiB           1   @profile(precision=4)\n",
      "    23                                         def q2_memory_pandas_map_reduce(file_path: str) -> List[Tuple[str, int]]:\n",
      "    24 155.2891 MiB   0.0000 MiB           2       tweets = pd.read_json(\n",
      "    25 155.2891 MiB   0.0000 MiB           1           file_path,\n",
      "    26 155.2891 MiB   0.0000 MiB           1           lines=True,\n",
      "    27 155.2891 MiB   0.0000 MiB           1           chunksize=1000,\n",
      "    28 155.2891 MiB   0.0000 MiB           1           convert_dates=True,\n",
      "    29 155.2891 MiB   0.0000 MiB          19           dtype={\n",
      "    30 155.2891 MiB   0.0000 MiB           1               \"url\": \"bool\",\n",
      "    31 155.2891 MiB   0.0000 MiB           1               \"date\": \"bool\",\n",
      "    32 155.2891 MiB   0.0000 MiB           1               \"content\": \"string\",\n",
      "    33 155.2891 MiB   0.0000 MiB           1               \"renderedContent\": \"bool\",\n",
      "    34 155.2891 MiB   0.0000 MiB           1               \"id\": \"bool\",\n",
      "    35 155.2891 MiB   0.0000 MiB           1               \"user\": \"bool\",\n",
      "    36 155.2891 MiB   0.0000 MiB           1               \"outlinks\": \"bool\",\n",
      "    37 155.2891 MiB   0.0000 MiB           1               \"tcooutlinks\": \"bool\",\n",
      "    38 155.2891 MiB   0.0000 MiB           1               \"replyCount\": \"bool\",\n",
      "    39 155.2891 MiB   0.0000 MiB           1               \"retweetCount\": \"bool\",\n",
      "    40 155.2891 MiB   0.0000 MiB           1               \"likeCount\": \"bool\",\n",
      "    41 155.2891 MiB   0.0000 MiB           1               \"quoteCount\": \"bool\",\n",
      "    42 155.2891 MiB   0.0000 MiB           1               \"conversationId\": \"bool\",\n",
      "    43 155.2891 MiB   0.0000 MiB           1               \"lang\": \"bool\",\n",
      "    44 155.2891 MiB   0.0000 MiB           1               \"source\": \"bool\",\n",
      "    45 155.2891 MiB   0.0000 MiB           1               \"sourceUrl\": \"bool\",\n",
      "    46 155.2891 MiB   0.0000 MiB           1               \"sourceLabel\": \"bool\",\n",
      "    47 155.2891 MiB   0.0000 MiB           1               \"media\": \"bool\",\n",
      "    48 155.2891 MiB   0.0000 MiB           1               \"retweetedTweet\": \"bool\",\n",
      "    49 155.2891 MiB   0.0000 MiB           1               \"quotedTweet\": \"bool\",\n",
      "    50 155.2891 MiB   0.0000 MiB           1               \"mentionedUsers\": \"bool\",\n",
      "    51                                                 },\n",
      "    52                                             )\n",
      "    53                                         \n",
      "    54 155.2891 MiB   0.0000 MiB           1       emoji_list = map(get_emojis, tweets)\n",
      "    55 184.9141 MiB  29.6250 MiB         235       result = reduce(lambda x,y: x+y, emoji_list)\n",
      "    56 184.9141 MiB   0.0000 MiB           1       counter_emojis = Counter(result)\n",
      "    57 184.9141 MiB   0.0000 MiB           1       top_emojis = counter_emojis.most_common(10)\n",
      "    58 184.9141 MiB   0.0000 MiB           1       return top_emojis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory_pandas_map_reduce\n",
    "result = q2_memory_pandas_map_reduce(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ™', 5049),\n",
       " ('ğŸ˜‚', 3072),\n",
       " ('ğŸšœ', 2972),\n",
       " ('ğŸŒ¾', 2182),\n",
       " ('ğŸ‡®ğŸ‡³', 2086),\n",
       " ('ğŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ğŸ™ğŸ»', 1317),\n",
       " ('ğŸ’š', 1040)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.2 s, sys: 986 ms, total: 47.2 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q2_memory import q2_memory_pandas_map_reduce\n",
    "\n",
    "result = q2_memory_pandas_map_reduce(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ™', 5049),\n",
       " ('ğŸ˜‚', 3072),\n",
       " ('ğŸšœ', 2972),\n",
       " ('ğŸŒ¾', 2182),\n",
       " ('ğŸ‡®ğŸ‡³', 2086),\n",
       " ('ğŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ğŸ™ğŸ»', 1317),\n",
       " ('ğŸ’š', 1040)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 228 ms, total: 29.3 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q2_memory import q2_memory\n",
    "result = q2_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ™', 5049),\n",
       " ('ğŸ˜‚', 3072),\n",
       " ('ğŸšœ', 2972),\n",
       " ('ğŸŒ¾', 2182),\n",
       " ('ğŸ‡®ğŸ‡³', 2086),\n",
       " ('ğŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ğŸ™ğŸ»', 1317),\n",
       " ('ğŸ’š', 1040)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
