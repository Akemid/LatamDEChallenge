{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEER CHALLENGE\n",
    "\n",
    "Para el desarrollo de este challenge estoy utilizando Python 3.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomprimir el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import unzip_json\n",
    "file_path = \"datasets/tweets.json.zip\"\n",
    "output_dir = \"datasets/\"\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    unzip_json(file_path,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solución de las preguntas de optimización de memoria se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar la memoria más no mantenerla al minimo.\n",
    "2. El tiempo de ejecución no debería ser excesivo aunque se este buscando optimizar la memoria.\n",
    "3. El archivo es un ejemplo pero pueden haber casos dónde sean muy extensos.\n",
    "\n",
    "Como solución se ha considerado utilizar la librería Pandas debido a su fácil manejo y que nos provee agregar argumentos a la función de lectura del archivo JSON para poder optimizar la memoría. Por otro lado, se manejó la alternativa de utilizar una lectura linea a linea del archivo; sin embargo, esto tomaría demasiado tiempo de ejecución en relación al beneficio de ahorrar memoría.\n",
    "\n",
    "Consideraciones claves para reducir el consumo de memoría con Pandas:\n",
    "\n",
    "1. Chunksize: Nos permite dividir archivos grandes en pequeños trozos lo cual permite el ahorro de memoría.\n",
    "2. dtype: Nos permite definir los tipos de datos de cada columna, así se puede asignar tipos de datos menores a los de por defecto. En este caso, tambien se uso para convertir información no necesaria en un tipo de dato booleano ( dado que cuesta mucho menos).\n",
    "3. Elegir las columnas necesarias para el proceso.\n",
    "4. Utilizar funciones map y reduce para reducir el uso de memoría y trabajar con los chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solución de las preguntas de optimizacion de tiempo se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar el tiempo de ejecución sin dejar de lado el consumo de memoria\n",
    "2. Limites de hardware propio.\n",
    "3. La data suministrada no será muy extensa para considerarse BigData.\n",
    "\n",
    "Como solución para las preguntas de optimización de tiempo se ha considerado entre dos alternativas la utilización de la libreria Polars y la lectura de cada linea utilizando la libreria JSON. Sabemos que para optimizar el tiempo de ejecución es necesario tambien tener en cuenta la memoria consumida, además de la escritura de código eficiente. Por otro lado, la paralelización de las tareas ayuda a acelerar el proceso aunque en Python es complicado debido al GIL existente. Asimismo, el lenguaje interpretado como tal es más lento que lenguajes compiladas como Rust o C++, por tal motivo, para acelerar la ejecución se han buscado opciones de librerias que tengan una API para python. Por ultimo, se ha tenido en cuenta que las operaciones vectoriales también aumentan la velocidad de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por todo lo anterior descrito, Polars es la mejor alternativa para optimizar la lectura de los dataframes (según las consideraciones descritas más arriba).\n",
    "\n",
    "Consideraciones de [Polars](https://docs.pola.rs/):\n",
    "\n",
    "1. Escrito en Rust.\n",
    "2. Trabajo en paralelo.\n",
    "3. Motor de Consultas Vectorizadas.\n",
    "\n",
    "Consideraciones del código:\n",
    "\n",
    "1. Se ha escrito el codigo utilizando lazy API puesto que permite trabajar con datasets grandes y en streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8 137.3125 MiB 137.3125 MiB           1   @profile(precision=4)\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             # The selected function is the one that has the best performance,\n",
      "    11                                             # It is the one that uses pandas with the best memory optimization.\n",
      "    12                                             # The line by line function is the one that uses the least memory but it is the slowest\n",
      "    13                                             # And for that reason it is not the best option.\n",
      "    14                                             \n",
      "    15                                             # OPTIMIZATION:\n",
      "    16                                             # Many options to optimize the memory\n",
      "    17                                             # Chunksize allows to read the file in chunks\n",
      "    18                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    19                                             # in order to save memory\n",
      "    20 137.4375 MiB   0.1250 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=1000,convert_dates=True,dtype={\n",
      "    21 137.3125 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    22 137.3125 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    23 137.3125 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    24 137.3125 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    25 137.3125 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    26 137.3125 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    27 137.3125 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    28 137.3125 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    29 137.3125 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    30 137.3125 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    31 137.3125 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    32 137.3125 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    33 137.3125 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    34 137.3125 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    35 137.3125 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    36 137.3125 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    37 137.3125 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    38 137.3125 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    39 137.3125 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    40 137.3125 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    41 137.3125 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    42                                             })\n",
      "    43                                             # Each chunk is processed and then added to the result\n",
      "    44 137.4375 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    45                                             # The result is reduced to a single dataframe\n",
      "    46 171.9766 MiB  34.5391 MiB           1       result = reduce(add, procced_tweets)\n",
      "    47                                             # The top dates are extracted\n",
      "    48 172.2266 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    49 172.2266 MiB   0.0000 MiB           1       users = []\n",
      "    50 172.2266 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    51                                             # The top user for each date is extracted \n",
      "    52 172.2266 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    53                                                 # Filter by date\n",
      "    54 172.2266 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    55                                                 # Get the sum of the tweets for each user in the date\n",
      "    56 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    57                                                 # Get the user with the most tweets\n",
      "    58 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "    59                                                 # Get the username\n",
      "    60 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "    61 172.2266 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "    62 172.2266 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory\n",
    "result = q1_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 TIME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 s, sys: 789 ms, total: 5.95 s\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q1_time import q1_time\n",
    "result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.69 s ± 285 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q1_time import q1_time\n",
    "%timeit result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los top 10 emojis más usados con su respectivo conteo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver esta pregunta se ha utilizado la librería [Emoji](https://carpedm20.github.io/emoji/docs/).\n",
    "Hubieron otras alternativas cómo hacer web scrapping de la página dónde estan almacenados los caracteres [unicode.org](https://www.unicode.org/reports/tr51/#emoji_data); sin embargo, para opciones prácticas y no reinventar la rueda se procedió a utilizar la librería Emoji. Además en la búsqueda de los emojis se consideró por un momento utilizar expresiones regulares, pero podían aparecer errores como esta detallado [aquí](https://carpedm20.github.io/emoji/docs/#regular-expression). Por lo tanto, se utilizo la función emoji_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     9 145.3945 MiB 145.3945 MiB           1   @profile(precision=4)\n",
      "    10                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    11 145.5195 MiB   0.1250 MiB           2       tweets = pd.read_json(\n",
      "    12 145.3945 MiB   0.0000 MiB           1           file_path,\n",
      "    13 145.3945 MiB   0.0000 MiB           1           lines=True,\n",
      "    14 145.3945 MiB   0.0000 MiB           1           chunksize=1000,\n",
      "    15 145.3945 MiB   0.0000 MiB           1           convert_dates=True,\n",
      "    16 145.3945 MiB   0.0000 MiB          19           dtype={\n",
      "    17 145.3945 MiB   0.0000 MiB           1               \"url\": \"bool\",\n",
      "    18 145.3945 MiB   0.0000 MiB           1               \"date\": \"bool\",\n",
      "    19 145.3945 MiB   0.0000 MiB           1               \"content\": \"string\",\n",
      "    20 145.3945 MiB   0.0000 MiB           1               \"renderedContent\": \"bool\",\n",
      "    21 145.3945 MiB   0.0000 MiB           1               \"id\": \"bool\",\n",
      "    22 145.3945 MiB   0.0000 MiB           1               \"user\": \"bool\",\n",
      "    23 145.3945 MiB   0.0000 MiB           1               \"outlinks\": \"bool\",\n",
      "    24 145.3945 MiB   0.0000 MiB           1               \"tcooutlinks\": \"bool\",\n",
      "    25 145.3945 MiB   0.0000 MiB           1               \"replyCount\": \"bool\",\n",
      "    26 145.3945 MiB   0.0000 MiB           1               \"retweetCount\": \"bool\",\n",
      "    27 145.3945 MiB   0.0000 MiB           1               \"likeCount\": \"bool\",\n",
      "    28 145.3945 MiB   0.0000 MiB           1               \"quoteCount\": \"bool\",\n",
      "    29 145.3945 MiB   0.0000 MiB           1               \"conversationId\": \"bool\",\n",
      "    30 145.3945 MiB   0.0000 MiB           1               \"lang\": \"bool\",\n",
      "    31 145.3945 MiB   0.0000 MiB           1               \"source\": \"bool\",\n",
      "    32 145.3945 MiB   0.0000 MiB           1               \"sourceUrl\": \"bool\",\n",
      "    33 145.3945 MiB   0.0000 MiB           1               \"sourceLabel\": \"bool\",\n",
      "    34 145.3945 MiB   0.0000 MiB           1               \"media\": \"bool\",\n",
      "    35 145.3945 MiB   0.0000 MiB           1               \"retweetedTweet\": \"bool\",\n",
      "    36 145.3945 MiB   0.0000 MiB           1               \"quotedTweet\": \"bool\",\n",
      "    37 145.3945 MiB   0.0000 MiB           1               \"mentionedUsers\": \"bool\",\n",
      "    38                                                 },\n",
      "    39                                             )\n",
      "    40                                         \n",
      "    41 145.5195 MiB   0.0000 MiB           1       emoji_list = map(get_emojis, tweets)\n",
      "    42 179.8945 MiB  34.3750 MiB         235       result = reduce(lambda x,y: x+y, emoji_list)\n",
      "    43 179.8945 MiB   0.0000 MiB           1       counter_emojis = Counter(result)\n",
      "    44 179.8945 MiB   0.0000 MiB           1       top_emojis = counter_emojis.most_common(10)\n",
      "    45 179.8945 MiB   0.0000 MiB           1       return top_emojis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q2_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
