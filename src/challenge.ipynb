{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci√≥n y todas las suposiciones que est√°s considerando. Aqu√≠ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEER CHALLENGE\n",
    "\n",
    "Para el desarrollo de este challenge estoy utilizando Python 3.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomprimir el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import unzip_json\n",
    "file_path = \"datasets/tweets.json.zip\"\n",
    "output_dir = \"datasets/\"\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    unzip_json(file_path,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la soluci√≥n de las preguntas de optimizaci√≥n de memoria se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar la memoria m√°s no mantenerla al minimo.\n",
    "2. El tiempo de ejecuci√≥n no deber√≠a ser excesivo aunque se este buscando optimizar la memoria.\n",
    "3. El archivo es un ejemplo pero pueden haber casos d√≥nde sean muy extensos.\n",
    "\n",
    "Como soluci√≥n se ha considerado utilizar la librer√≠a Pandas debido a su f√°cil manejo y que nos provee agregar argumentos a la funci√≥n de lectura del archivo JSON para poder optimizar la memor√≠a. Por otro lado, se manej√≥ la alternativa de utilizar una lectura linea a linea del archivo; sin embargo, esto tomar√≠a demasiado tiempo de ejecuci√≥n en relaci√≥n al beneficio de ahorrar memor√≠a.\n",
    "\n",
    "Consideraciones claves para reducir el consumo de memor√≠a con Pandas:\n",
    "\n",
    "1. Chunksize: Nos permite dividir archivos grandes en peque√±os trozos lo cual permite el ahorro de memor√≠a.\n",
    "2. dtype: Nos permite definir los tipos de datos de cada columna, as√≠ se puede asignar tipos de datos menores a los de por defecto. En este caso, tambien se uso para convertir informaci√≥n no necesaria en un tipo de dato booleano ( dado que cuesta mucho menos).\n",
    "3. Elegir las columnas necesarias para el proceso.\n",
    "4. Utilizar funciones map y reduce para reducir el uso de memor√≠a y trabajar con los chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaciones de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la soluci√≥n de las preguntas de optimizacion de tiempo se ha considerado lo siguiente:\n",
    "\n",
    "1. Se busca optimizar el tiempo de ejecuci√≥n sin dejar de lado el consumo de memoria\n",
    "2. Limites de hardware propio.\n",
    "3. La data suministrada no ser√° muy extensa para considerarse BigData.\n",
    "\n",
    "Como soluci√≥n para las preguntas de optimizaci√≥n de tiempo se ha considerado entre dos alternativas la utilizaci√≥n de la libreria Polars y la lectura de cada linea utilizando la libreria JSON. Sabemos que para optimizar el tiempo de ejecuci√≥n es necesario tambien tener en cuenta la memoria consumida, adem√°s de la escritura de c√≥digo eficiente. Por otro lado, la paralelizaci√≥n de las tareas ayuda a acelerar el proceso aunque en Python es complicado debido al GIL existente. Asimismo, el lenguaje interpretado como tal es m√°s lento que lenguajes compiladas como Rust o C++, por tal motivo, para acelerar la ejecuci√≥n se han buscado opciones de librerias que tengan una API para python. Por ultimo, se ha tenido en cuenta que las operaciones vectoriales tambi√©n aumentan la velocidad de ejecuci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por todo lo anterior descrito, Polars es la mejor alternativa para optimizar la lectura de los dataframes (seg√∫n las consideraciones descritas m√°s arriba).\n",
    "\n",
    "Consideraciones de [Polars](https://docs.pola.rs/):\n",
    "\n",
    "1. Escrito en Rust.\n",
    "2. Trabajo en paralelo.\n",
    "3. Motor de Consultas Vectorizadas.\n",
    "\n",
    "Consideraciones del c√≥digo:\n",
    "\n",
    "1. Se ha escrito el codigo utilizando lazy API puesto que permite trabajar con datasets grandes y en streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8 137.3125 MiB 137.3125 MiB           1   @profile(precision=4)\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             # The selected function is the one that has the best performance,\n",
      "    11                                             # It is the one that uses pandas with the best memory optimization.\n",
      "    12                                             # The line by line function is the one that uses the least memory but it is the slowest\n",
      "    13                                             # And for that reason it is not the best option.\n",
      "    14                                             \n",
      "    15                                             # OPTIMIZATION:\n",
      "    16                                             # Many options to optimize the memory\n",
      "    17                                             # Chunksize allows to read the file in chunks\n",
      "    18                                             # Dtypes allows to specify the type of the columns and loss information that is not needed\n",
      "    19                                             # in order to save memory\n",
      "    20 137.4375 MiB   0.1250 MiB          19       tweets = pd.read_json(file_path, lines=True,chunksize=1000,convert_dates=True,dtype={\n",
      "    21 137.3125 MiB   0.0000 MiB           1           \"url\":\"bool\",\n",
      "    22 137.3125 MiB   0.0000 MiB           1           \"date\":\"datetime64[ns]\",\n",
      "    23 137.3125 MiB   0.0000 MiB           1           \"content\":\"bool\",\n",
      "    24 137.3125 MiB   0.0000 MiB           1           \"renderedContent\": \"bool\",\n",
      "    25 137.3125 MiB   0.0000 MiB           1           \"id\": \"bool\",\n",
      "    26 137.3125 MiB   0.0000 MiB           1           \"user\":\"object\",\n",
      "    27 137.3125 MiB   0.0000 MiB           1           \"outlinks\":\"bool\",\n",
      "    28 137.3125 MiB   0.0000 MiB           1           \"tcooutlinks\":\"bool\",\n",
      "    29 137.3125 MiB   0.0000 MiB           1           \"replyCount\":\"bool\",\n",
      "    30 137.3125 MiB   0.0000 MiB           1           \"retweetCount\":\"bool\",\n",
      "    31 137.3125 MiB   0.0000 MiB           1           \"likeCount\":\"bool\",\n",
      "    32 137.3125 MiB   0.0000 MiB           1           \"quoteCount\":\"bool\",\n",
      "    33 137.3125 MiB   0.0000 MiB           1           \"conversationId\":\"bool\",\n",
      "    34 137.3125 MiB   0.0000 MiB           1           \"lang\":\"bool\",\n",
      "    35 137.3125 MiB   0.0000 MiB           1           \"source\":\"bool\",\n",
      "    36 137.3125 MiB   0.0000 MiB           1           \"sourceUrl\":\"bool\",\n",
      "    37 137.3125 MiB   0.0000 MiB           1           \"sourceLabel\":\"bool\",\n",
      "    38 137.3125 MiB   0.0000 MiB           1           \"media\":\"bool\",\n",
      "    39 137.3125 MiB   0.0000 MiB           1           \"retweetedTweet\":\"bool\",\n",
      "    40 137.3125 MiB   0.0000 MiB           1           \"quotedTweet\":\"bool\",\n",
      "    41 137.3125 MiB   0.0000 MiB           1           \"mentionedUsers\":\"bool\",\n",
      "    42                                             })\n",
      "    43                                             # Each chunk is processed and then added to the result\n",
      "    44 137.4375 MiB   0.0000 MiB           1       procced_tweets = map(process_tweets, tweets)\n",
      "    45                                             # The result is reduced to a single dataframe\n",
      "    46 171.9766 MiB  34.5391 MiB           1       result = reduce(add, procced_tweets)\n",
      "    47                                             # The top dates are extracted\n",
      "    48 172.2266 MiB   0.2500 MiB           1       top_dates = result.groupby(['date']).sum().sort_values(ascending=False).head(10).index.values.tolist()\n",
      "    49 172.2266 MiB   0.0000 MiB           1       users = []\n",
      "    50 172.2266 MiB   0.0000 MiB           1       result = result.reset_index()   \n",
      "    51                                             # The top user for each date is extracted \n",
      "    52 172.2266 MiB   0.0000 MiB          11       for date in top_dates:\n",
      "    53                                                 # Filter by date\n",
      "    54 172.2266 MiB   0.0000 MiB          10           top_twitter_user = result[result['date'] == date][[\"user\",0]]\n",
      "    55                                                 # Get the sum of the tweets for each user in the date\n",
      "    56 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.groupby(['user']).sum()    \n",
      "    57                                                 # Get the user with the most tweets\n",
      "    58 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.sort_values(by=0,ascending=False).head(1)\n",
      "    59                                                 # Get the username\n",
      "    60 172.2266 MiB   0.0000 MiB          10           top_twitter_user = top_twitter_user.index.values.tolist()[0]\n",
      "    61 172.2266 MiB   0.0000 MiB          10           users.append(top_twitter_user)\n",
      "    62 172.2266 MiB   0.0000 MiB           1       return list(zip(top_dates, users)) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory\n",
    "result = q1_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 TIME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 s, sys: 789 ms, total: 5.95 s\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from q1_time import q1_time\n",
    "result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.69 s ¬± 285 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q1_time import q1_time\n",
    "%timeit result = q1_time(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los top 10 emojis m√°s usados con su respectivo conteo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver esta pregunta se ha utilizado la librer√≠a [Emoji](https://carpedm20.github.io/emoji/docs/).\n",
    "Hubieron otras alternativas c√≥mo hacer web scrapping de la p√°gina d√≥nde estan almacenados los caracteres [unicode.org](https://www.unicode.org/reports/tr51/#emoji_data); sin embargo, para opciones pr√°cticas y no reinventar la rueda se procedi√≥ a utilizar la librer√≠a Emoji. Adem√°s en la b√∫squeda de los emojis se consider√≥ por un momento utilizar expresiones regulares, pero pod√≠an aparecer errores como esta detallado [aqu√≠](https://carpedm20.github.io/emoji/docs/#regular-expression). Por lo tanto, se utilizo la funci√≥n emoji_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/admintdp/Documentos/personalProyects/challenge_DE/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     9 145.3945 MiB 145.3945 MiB           1   @profile(precision=4)\n",
      "    10                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    11 145.5195 MiB   0.1250 MiB           2       tweets = pd.read_json(\n",
      "    12 145.3945 MiB   0.0000 MiB           1           file_path,\n",
      "    13 145.3945 MiB   0.0000 MiB           1           lines=True,\n",
      "    14 145.3945 MiB   0.0000 MiB           1           chunksize=1000,\n",
      "    15 145.3945 MiB   0.0000 MiB           1           convert_dates=True,\n",
      "    16 145.3945 MiB   0.0000 MiB          19           dtype={\n",
      "    17 145.3945 MiB   0.0000 MiB           1               \"url\": \"bool\",\n",
      "    18 145.3945 MiB   0.0000 MiB           1               \"date\": \"bool\",\n",
      "    19 145.3945 MiB   0.0000 MiB           1               \"content\": \"string\",\n",
      "    20 145.3945 MiB   0.0000 MiB           1               \"renderedContent\": \"bool\",\n",
      "    21 145.3945 MiB   0.0000 MiB           1               \"id\": \"bool\",\n",
      "    22 145.3945 MiB   0.0000 MiB           1               \"user\": \"bool\",\n",
      "    23 145.3945 MiB   0.0000 MiB           1               \"outlinks\": \"bool\",\n",
      "    24 145.3945 MiB   0.0000 MiB           1               \"tcooutlinks\": \"bool\",\n",
      "    25 145.3945 MiB   0.0000 MiB           1               \"replyCount\": \"bool\",\n",
      "    26 145.3945 MiB   0.0000 MiB           1               \"retweetCount\": \"bool\",\n",
      "    27 145.3945 MiB   0.0000 MiB           1               \"likeCount\": \"bool\",\n",
      "    28 145.3945 MiB   0.0000 MiB           1               \"quoteCount\": \"bool\",\n",
      "    29 145.3945 MiB   0.0000 MiB           1               \"conversationId\": \"bool\",\n",
      "    30 145.3945 MiB   0.0000 MiB           1               \"lang\": \"bool\",\n",
      "    31 145.3945 MiB   0.0000 MiB           1               \"source\": \"bool\",\n",
      "    32 145.3945 MiB   0.0000 MiB           1               \"sourceUrl\": \"bool\",\n",
      "    33 145.3945 MiB   0.0000 MiB           1               \"sourceLabel\": \"bool\",\n",
      "    34 145.3945 MiB   0.0000 MiB           1               \"media\": \"bool\",\n",
      "    35 145.3945 MiB   0.0000 MiB           1               \"retweetedTweet\": \"bool\",\n",
      "    36 145.3945 MiB   0.0000 MiB           1               \"quotedTweet\": \"bool\",\n",
      "    37 145.3945 MiB   0.0000 MiB           1               \"mentionedUsers\": \"bool\",\n",
      "    38                                                 },\n",
      "    39                                             )\n",
      "    40                                         \n",
      "    41 145.5195 MiB   0.0000 MiB           1       emoji_list = map(get_emojis, tweets)\n",
      "    42 179.8945 MiB  34.3750 MiB         235       result = reduce(lambda x,y: x+y, emoji_list)\n",
      "    43 179.8945 MiB   0.0000 MiB           1       counter_emojis = Counter(result)\n",
      "    44 179.8945 MiB   0.0000 MiB           1       top_emojis = counter_emojis.most_common(10)\n",
      "    45 179.8945 MiB   0.0000 MiB           1       return top_emojis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory\n",
    "output_file_path = \"datasets/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q2_memory(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üôè', 5049),\n",
       " ('üòÇ', 3072),\n",
       " ('üöú', 2972),\n",
       " ('üåæ', 2182),\n",
       " ('üáÆüá≥', 2086),\n",
       " ('ü§£', 1668),\n",
       " ('‚úä', 1651),\n",
       " ('‚ù§Ô∏è', 1382),\n",
       " ('üôèüèª', 1317),\n",
       " ('üíö', 1040)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
